{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "#import plotting functions\n",
    "from nilearn.plotting import view_img, plot_anat\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "import re \n",
    "from tensorflow.python.platform import gfile\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'n'\n",
    "images_dir = 'CN_grid/'\n",
    "images_dir1 = 'AD_grid/'\n",
    "list_CN = [images_dir+f for f in os.listdir(images_dir ) if re.search('PNG|png', f)]\n",
    "list_AD = [images_dir1+f for f in os.listdir(images_dir1 ) if re.search('PNG|png', f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_img = list_CN + list_AD\n",
    "len(list_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph():\n",
    "    with tf.compat.v2.io.gfile.GFile(os.path.join(model_dir, 'classify_image_graph_def.pb'), 'rb')  as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_tf(image_path):\n",
    "    nb_features = 2048\n",
    "    \n",
    "    features = np.empty((len(list_img),nb_features))\n",
    "    labels = []\n",
    "    print(len(list_img))\n",
    "    create_graph()\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        next_to_last_tensor = sess.graph.get_tensor_by_name('pool_3/_reshape:0')\n",
    "        \n",
    "        for ind, image in enumerate(list_img):\n",
    "            if (ind%100 == 0):\n",
    "                print('Processing %s...' % (image))\n",
    "            \n",
    "\n",
    "            image_data = tf.compat.v1.gfile.FastGFile(image, 'rb').read()\n",
    "            predictions = sess.run(next_to_last_tensor, {'DecodeJpeg/contents:0': image_data})\n",
    "            features[ind,:] = np.squeeze(predictions)\n",
    "            labels.append(re.split('_\\d+',image.split('/')[1])[0])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_keras(image_path):\n",
    "    base_model = InceptionV3(include_top=False, pooling = 'avg',weights='imagenet',input_shape=(474,570,3))\n",
    "    #model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg').output)\n",
    "    pred = []\n",
    "    for imge in image_path:\n",
    "        #print(imge)\n",
    "        img = image.load_img(imge)\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        predictions = base_model.predict(x)\n",
    "        pred.append(np.squeeze(predictions))\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6052\n",
      "Processing 2d/CN_side149_2.png...\n",
      "WARNING:tensorflow:From <ipython-input-254-868d11590d22>:16: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "Processing 2d/AD_side319_2.png...\n",
      "Processing 2d/CN_top162_2.png...\n",
      "Processing 2d/CN_top42_3.png...\n",
      "Processing 2d/CN_back55_2.png...\n",
      "Processing 2d/AD_side56_2.png...\n",
      "Processing 2d/AD_side319_3.png...\n",
      "Processing 2d/AD_side118_1.png...\n",
      "Processing 2d/AD_top143_1.png...\n",
      "Processing 2d/CN_side67_3.png...\n",
      "Processing 2d/CN_side109_3.png...\n",
      "Processing 2d/CN_side226_3.png...\n",
      "Processing 2d/AD_back294_2.png...\n",
      "Processing 2d/AD_back245_1.png...\n",
      "Processing 2d/AD_top109_2.png...\n",
      "Processing 2d/AD_top203_1.png...\n",
      "Processing 2d/CN_top267_1.png...\n",
      "Processing 2d/CN_back297_1.png...\n",
      "Processing 2d/CN_side325_2.png...\n",
      "Processing 2d/AD_top204_2.png...\n",
      "Processing 2d/AD_back149_2.png...\n",
      "Processing 2d/CN_side174_2.png...\n",
      "Processing 2d/CN_side129_1.png...\n",
      "Processing 2d/AD_top198_3.png...\n",
      "Processing 2d/CN_side39_3.png...\n",
      "Processing 2d/CN_back250_2.png...\n",
      "Processing 2d/AD_side49_2.png...\n",
      "Processing 2d/AD_side92_1.png...\n",
      "Processing 2d/CN_top69_2.png...\n",
      "Processing 2d/CN_back7_1.png...\n",
      "Processing 2d/AD_back267_2.png...\n",
      "Processing 2d/CN_top205_1.png...\n",
      "Processing 2d/CN_side319_3.png...\n",
      "Processing 2d/CN_side279_2.png...\n",
      "Processing 2d/CN_side197_2.png...\n",
      "Processing 2d/CN_side334_1.png...\n",
      "Processing 2d/AD_back228_3.png...\n",
      "Processing 2d/CN_top119_2.png...\n",
      "Processing 2d/AD_top249_1.png...\n",
      "Processing 2d/AD_side71_2.png...\n",
      "Processing 2d/CN_top10_1.png...\n",
      "Processing 2d/AD_top105_1.png...\n",
      "Processing 2d/AD_top124_2.png...\n",
      "Processing 2d/CN_top20_3.png...\n",
      "Processing 2d/CN_back86_1.png...\n",
      "Processing 2d/CN_top311_1.png...\n",
      "Processing 2d/CN_side61_2.png...\n",
      "Processing 2d/AD_top145_3.png...\n",
      "Processing 2d/AD_top250_3.png...\n",
      "Processing 2d/AD_top170_2.png...\n",
      "Processing 2d/CN_back190_1.png...\n",
      "Processing 2d/AD_top142_2.png...\n",
      "Processing 2d/CN_top120_3.png...\n",
      "Processing 2d/CN_side71_2.png...\n",
      "Processing 2d/CN_back105_2.png...\n",
      "Processing 2d/AD_top39_1.png...\n",
      "Processing 2d/AD_back278_2.png...\n",
      "Processing 2d/CN_side45_1.png...\n",
      "Processing 2d/CN_top42_2.png...\n",
      "Processing 2d/AD_back85_1.png...\n",
      "Processing 2d/CN_back116_3.png...\n"
     ]
    }
   ],
   "source": [
    "features,labelss = extract_features_tf(list_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_f = extract_features_keras(list_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder array\n",
    "labels= np.zeros(len(b_f))\n",
    "#we know that the sick brains come after the normal ones since we just concatenated the health with the sick brains\n",
    "#But your code in the bracket[]\n",
    "labels[346:685] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8160382f898f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "b_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(features, open('features', 'wb'))\n",
    "pickle.dump(labels, open('labels', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/a/aimkha18/miniconda3/envs/deep/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(C=1.0, loss='squared_hinge', penalty='l2',multi_class='ovr')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true,y_pred):\n",
    "    cm_array = confusion_matrix(y_true,y_pred)\n",
    "    true_labels = np.unique(y_true)\n",
    "    pred_labels = np.unique(y_pred)\n",
    "    plt.imshow(cm_array[:-1,:-1], interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\", fontsize=16)\n",
    "    cbar = plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Number of images', rotation=270, labelpad=30, fontsize=12)\n",
    "    xtick_marks = np.arange(len(true_labels))\n",
    "    ytick_marks = np.arange(len(pred_labels))\n",
    "    plt.xticks(xtick_marks, true_labels, rotation=90)\n",
    "    plt.yticks(ytick_marks,pred_labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=14)\n",
    "    plt.xlabel('Predicted label', fontsize=14)\n",
    "    fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 12\n",
    "    fig_size[1] = 12\n",
    "    plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "    sens = TP / (TP + FN) \n",
    "    spec = TN / (FP + TN)\n",
    "    print (\"Sensitivity: \", sens, \"Specificity:\", spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.6%\n",
      "Sensitivity:  0.6607142857142857 Specificity: 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Accuracy: {0:0.1f}%\".format(accuracy_score(y_test,y_pred)*100))\n",
    "perf_measure(y_test,y_pred)\n",
    "#plot_confusion_matrix(y_test,y_pred)\n",
    "#sensitivity = TP / (TP + FN) 96.30%\n",
    "#specificity = TN / (FP + TN) 99.58%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.781021897810219"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn import svm\n",
    "#from sklearn import cross_validation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=10).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/a/aimkha18/miniconda3/envs/deep/lib/python3.7/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.635080988917306]\n",
      "[0.635080988917306, 0.7630434782608696]\n",
      "[0.635080988917306, 0.7630434782608696, 0.7792838874680307]\n",
      "[0.635080988917306, 0.7630434782608696, 0.7792838874680307, 0.7633205456095482]\n",
      "[0.635080988917306, 0.7630434782608696, 0.7792838874680307, 0.7633205456095482, 0.7648337595907928]\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n_splits=10, shuffle=False, random_state=0)\n",
    "C_array=[0.001,0.01,0.1,1,10]\n",
    "C_scores=[]\n",
    "\n",
    "for k in C_array:\n",
    "    clf = svm.SVC(kernel='linear', C=k)\n",
    "    scores= cross_val_score(clf, features, labels, cv=k_fold, n_jobs=-1)\n",
    "    C_scores.append(scores.mean())\n",
    "    print (C_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', C=0.1,probability=True)\n",
    "\n",
    "# final_model = clf.fit(features, labels)\n",
    "\n",
    "final_model = CalibratedClassifierCV(clf,cv=10,method='sigmoid')\n",
    "final_model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.3%\n",
      "Sensitivity:  0.6428571428571429 Specificity: 0.9135802469135802\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {0:0.1f}%\".format(accuracy_score(y_test,y_pred)*100))\n",
    "perf_measure(y_test,y_pred)\n",
    "#plot_confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=1000, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='sqrt',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual class predictions\n",
    "rf_predictions = model.predict(X_test)\n",
    "# Probabilities for each class\n",
    "rf_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#We make class predictions (predict) as well as predicted probabilities (predict_proba) \n",
    "#to calculate the ROC AUC. Once we have the testing predictions, we can calculate the ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_value = roc_auc_score(y_test, rf_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8477733686067019\n"
     ]
    }
   ],
   "source": [
    "print(roc_value)\n",
    "#print(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.6%\n",
      "Sensitivity:  0.7142857142857143 Specificity: 0.8518518518518519\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {0:0.1f}%\".format(accuracy_score(y_test,rf_predictions)*100))\n",
    "#plot_confusion_matrix(y_test,rf_predictions)\n",
    "perf_measure(y_test,rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7810219  0.79562044 0.76642336 0.82481752 0.78832117]\n",
      "0.7912408759124088\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(model, features, labels, scoring='accuracy', cv=5)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying another version pretrained network inceptionv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://colab.research.google.com/github/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part3.ipynb?utm_source=practicum-IC&utm_campaign=colab-external&utm_medium=referral&hl=en&utm_content=imageexercise3-colab\n",
    "local_weights_file = '/home/stud1/a/aimkha18/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "pre_trained_model = InceptionV3(input_shape=(87, 87, 3), include_top=False, weights=None)\n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "#pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pre_trained_model.layers:\n",
    "    #print (layer)\n",
    "    layer.trainable = False\n",
    "#pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer output shape: (None, 3, 3, 768)\n",
      "Tensor(\"mixed7_6/Identity:0\", shape=(None, 3, 3, 768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "print('last layer output shape:', last_layer.output_shape)\n",
    "last_output = last_layer.output\n",
    "print(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 1, 1, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 21,804,833\n",
      "Trainable params: 2,049\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_model = tf.keras.Sequential([\n",
    "  pre_trained_model,\n",
    "  keras.layers.GlobalAveragePooling2D(),\n",
    "  keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "base_model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.0001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/stud1/a/aimkha18/Thesis/tmp2/'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'CN')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'AD')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'CN')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'AD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN_back158_1.png\n",
      "['.ipynb_checkpoints', 'AD_back100_1.png', 'AD_back100_2.png', 'AD_back100_3.png', 'AD_back101_1.png', 'AD_back101_2.png', 'AD_back101_3.png', 'AD_back102_1.png', 'AD_back102_2.png', 'AD_back102_3.png']\n"
     ]
    }
   ],
   "source": [
    "train_cat_fnames = os.listdir(train_cats_dir)\n",
    "print(train_cat_fnames[1])\n",
    "\"\"\"img = load_img('/home/stud1/a/aimkha18/Thesis/tmp/train/CN/merge_top148.png')  # this is a PIL image\n",
    "x = img_to_array(img)\n",
    "print(x.shape)\"\"\"\n",
    "\n",
    "train_dog_fnames = os.listdir(train_dogs_dir)\n",
    "train_dog_fnames.sort()\n",
    "print(train_dog_fnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images: 2420\n",
      "total training dog images: 2421\n",
      "total validation cat images: 454\n",
      "total validation dog images: 454\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n",
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images_dir = \\'CN_grid/\\'\\nimages_dir1 = \\'AD_grid/\\'\\nlist_CN = [images_dir+\\'/\\'+f for f in os.listdir(images_dir) if re.search(\\'PNG|png\\', f)]\\nlist_AD = [images_dir1+\\'/\\'+f for f in os.listdir(images_dir1) if re.search(\\'PNG|png\\', f)]\\nprint(\"AD: \", len(list_AD), \"CN\", len(list_CN))\\nlist_X = list_CN+list_AD'"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"images_dir = 'CN_grid/'\n",
    "images_dir1 = 'AD_grid/'\n",
    "list_CN = [images_dir+'/'+f for f in os.listdir(images_dir) if re.search('PNG|png', f)]\n",
    "list_AD = [images_dir1+'/'+f for f in os.listdir(images_dir1) if re.search('PNG|png', f)]\n",
    "print(\"AD: \", len(list_AD), \"CN\", len(list_CN))\n",
    "list_X = list_CN+list_AD\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def load_all_data(list_X):\\n    data=[]\\n    temp=[]\\n    for imge in list_X:\\n        #print(imge)\\n        img = image.load_img(imge)\\n        temp = image.img_to_array(img)\\n        #print (np.expand_dims(temp,0),0)\\n     \\n        data.append(temp)\\n       \\n\\n    return data\\n\\n\\n\\n#train_x = loadedImages'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def load_all_data(list_X):\n",
    "    data=[]\n",
    "    temp=[]\n",
    "    for imge in list_X:\n",
    "        #print(imge)\n",
    "        img = image.load_img(imge)\n",
    "        temp = image.img_to_array(img)\n",
    "        #print (np.expand_dims(temp,0),0)\n",
    "     \n",
    "        data.append(temp)\n",
    "       \n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "#train_x = loadedImages\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load_all_data(list_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'labels= np.zeros(len(data))\\n#we know that the sick brains come after the normal ones since we just concatenated the health with the sick brains\\n#But your code in the bracket[]\\nlabels[346:685] = 1'"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"labels= np.zeros(len(data))\n",
    "#we know that the sick brains come after the normal ones since we just concatenated the health with the sick brains\n",
    "#But your code in the bracket[]\n",
    "labels[346:685] = 1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add our data-augmentation parameters to ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    #rotation_range=40,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    #shear_range=0.2,\n",
    "    zoom_range=0.08\n",
    "    #horizontal_flip=True\n",
    "    )\n",
    "\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4841 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "tr_batch = 33\n",
    "val_batch = 20\n",
    "tr_img = 4840\n",
    "val_img = 908\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir, # This is the source directory for training images\n",
    "        target_size=(87, 87),  # All images will be resized to 150x150\n",
    "        batch_size=tr_batch,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary',\n",
    "        shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 908 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Flow validation images in batches of 20 using val_datagen generator\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(87, 87),\n",
    "        batch_size=val_batch,\n",
    "        class_mode='binary',\n",
    "        shuffle = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AD': 0, 'CN': 1}\n"
     ]
    }
   ],
   "source": [
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "147/146 [==============================] - 207s 1s/step - loss: 0.7259 - accuracy: 0.4825 - val_loss: 0.7980 - val_accuracy: 0.5011\n",
      "Epoch 2/10\n",
      "147/146 [==============================] - 205s 1s/step - loss: 0.7101 - accuracy: 0.5024 - val_loss: 0.8012 - val_accuracy: 0.5077\n",
      "Epoch 3/10\n",
      "147/146 [==============================] - 204s 1s/step - loss: 0.7053 - accuracy: 0.5181 - val_loss: 0.8004 - val_accuracy: 0.5044\n",
      "Epoch 4/10\n",
      "147/146 [==============================] - 205s 1s/step - loss: 0.7015 - accuracy: 0.5259 - val_loss: 0.8004 - val_accuracy: 0.5099\n",
      "Epoch 5/10\n",
      "147/146 [==============================] - 205s 1s/step - loss: 0.6926 - accuracy: 0.5493 - val_loss: 0.8132 - val_accuracy: 0.5198\n",
      "Epoch 6/10\n",
      "147/146 [==============================] - 206s 1s/step - loss: 0.6940 - accuracy: 0.5352 - val_loss: 0.8178 - val_accuracy: 0.5231\n",
      "Epoch 7/10\n",
      "147/146 [==============================] - 206s 1s/step - loss: 0.6821 - accuracy: 0.5604 - val_loss: 0.8298 - val_accuracy: 0.5165\n",
      "Epoch 8/10\n",
      "147/146 [==============================] - 206s 1s/step - loss: 0.6859 - accuracy: 0.5584 - val_loss: 0.8161 - val_accuracy: 0.5209\n",
      "Epoch 9/10\n",
      "147/146 [==============================] - 205s 1s/step - loss: 0.6835 - accuracy: 0.5482 - val_loss: 0.8187 - val_accuracy: 0.5198\n",
      "Epoch 10/10\n",
      "147/146 [==============================] - 206s 1s/step - loss: 0.6810 - accuracy: 0.5602 - val_loss: 0.8279 - val_accuracy: 0.5242\n"
     ]
    }
   ],
   "source": [
    "history = base_model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=tr_img/tr_batch,\n",
    "      epochs=10,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=val_img/val_batch,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'validation_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-430-e405a3631cec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m211\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation_accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.figure(1, figsize=(20,10))  \n",
    "#odd. need to run above twice to set plot size\n",
    "plt.subplot(211)  \n",
    "plt.plot(history.history['accuracy'])  \n",
    "plt.plot(history.history['validation_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.subplot(212)  \n",
    "plt.plot(history.history['loss'])  \n",
    "plt.plot(history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper left')  \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed6 is now unfrozen\n",
      "conv2d_628 is now trainable\n",
      "batch_normalization_628 is now trainable\n",
      "activation_628 is now trainable\n",
      "conv2d_629 is now trainable\n",
      "batch_normalization_629 is now trainable\n",
      "activation_629 is now trainable\n",
      "conv2d_625 is now trainable\n",
      "conv2d_630 is now trainable\n",
      "batch_normalization_625 is now trainable\n",
      "batch_normalization_630 is now trainable\n",
      "activation_625 is now trainable\n",
      "activation_630 is now trainable\n",
      "conv2d_626 is now trainable\n",
      "conv2d_631 is now trainable\n",
      "batch_normalization_626 is now trainable\n",
      "batch_normalization_631 is now trainable\n",
      "activation_626 is now trainable\n",
      "activation_631 is now trainable\n",
      "average_pooling2d_60 is now trainable\n",
      "conv2d_624 is now trainable\n",
      "conv2d_627 is now trainable\n",
      "conv2d_632 is now trainable\n",
      "conv2d_633 is now trainable\n",
      "batch_normalization_624 is now trainable\n",
      "batch_normalization_627 is now trainable\n",
      "batch_normalization_632 is now trainable\n",
      "batch_normalization_633 is now trainable\n",
      "activation_624 is now trainable\n",
      "activation_627 is now trainable\n",
      "activation_632 is now trainable\n",
      "activation_633 is now trainable\n",
      "mixed7 is now trainable\n",
      "conv2d_636 is now trainable\n",
      "batch_normalization_636 is now trainable\n",
      "activation_636 is now trainable\n",
      "conv2d_637 is now trainable\n",
      "batch_normalization_637 is now trainable\n",
      "activation_637 is now trainable\n",
      "conv2d_634 is now trainable\n",
      "conv2d_638 is now trainable\n",
      "batch_normalization_634 is now trainable\n",
      "batch_normalization_638 is now trainable\n",
      "activation_634 is now trainable\n",
      "activation_638 is now trainable\n",
      "conv2d_635 is now trainable\n",
      "conv2d_639 is now trainable\n",
      "batch_normalization_635 is now trainable\n",
      "batch_normalization_639 is now trainable\n",
      "activation_635 is now trainable\n",
      "activation_639 is now trainable\n",
      "max_pooling2d_27 is now trainable\n",
      "mixed8 is now trainable\n",
      "conv2d_644 is now trainable\n",
      "batch_normalization_644 is now trainable\n",
      "activation_644 is now trainable\n",
      "conv2d_641 is now trainable\n",
      "conv2d_645 is now trainable\n",
      "batch_normalization_641 is now trainable\n",
      "batch_normalization_645 is now trainable\n",
      "activation_641 is now trainable\n",
      "activation_645 is now trainable\n",
      "conv2d_642 is now trainable\n",
      "conv2d_643 is now trainable\n",
      "conv2d_646 is now trainable\n",
      "conv2d_647 is now trainable\n",
      "average_pooling2d_61 is now trainable\n",
      "conv2d_640 is now trainable\n",
      "batch_normalization_642 is now trainable\n",
      "batch_normalization_643 is now trainable\n",
      "batch_normalization_646 is now trainable\n",
      "batch_normalization_647 is now trainable\n",
      "conv2d_648 is now trainable\n",
      "batch_normalization_640 is now trainable\n",
      "activation_642 is now trainable\n",
      "activation_643 is now trainable\n",
      "activation_646 is now trainable\n",
      "activation_647 is now trainable\n",
      "batch_normalization_648 is now trainable\n",
      "activation_640 is now trainable\n",
      "mixed9_0 is now trainable\n",
      "concatenate_12 is now trainable\n",
      "activation_648 is now trainable\n",
      "mixed9 is now trainable\n",
      "conv2d_653 is now trainable\n",
      "batch_normalization_653 is now trainable\n",
      "activation_653 is now trainable\n",
      "conv2d_650 is now trainable\n",
      "conv2d_654 is now trainable\n",
      "batch_normalization_650 is now trainable\n",
      "batch_normalization_654 is now trainable\n",
      "activation_650 is now trainable\n",
      "activation_654 is now trainable\n",
      "conv2d_651 is now trainable\n",
      "conv2d_652 is now trainable\n",
      "conv2d_655 is now trainable\n",
      "conv2d_656 is now trainable\n",
      "average_pooling2d_62 is now trainable\n",
      "conv2d_649 is now trainable\n",
      "batch_normalization_651 is now trainable\n",
      "batch_normalization_652 is now trainable\n",
      "batch_normalization_655 is now trainable\n",
      "batch_normalization_656 is now trainable\n",
      "conv2d_657 is now trainable\n",
      "batch_normalization_649 is now trainable\n",
      "activation_651 is now trainable\n",
      "activation_652 is now trainable\n",
      "activation_655 is now trainable\n",
      "activation_656 is now trainable\n",
      "batch_normalization_657 is now trainable\n",
      "activation_649 is now trainable\n",
      "mixed9_1 is now trainable\n",
      "concatenate_13 is now trainable\n",
      "activation_657 is now trainable\n",
      "mixed10 is now trainable\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "unfreeze = False\n",
    "\n",
    "# Unfreeze all models after \"mixed6\"\n",
    "for layer in pre_trained_model.layers:\n",
    "  if unfreeze:\n",
    "    layer.trainable = True\n",
    "    print(layer.name, \"is now trainable\")\n",
    "  if layer.name == 'mixed6':\n",
    "    unfreeze = True\n",
    "    print(layer.name, \"is now unfrozen\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an optimizer, here we will use SGD \n",
    "# with a very low learning rate (0.00001)\n",
    "base_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(\n",
    "                  lr=0.00001, \n",
    "                  momentum=0.9),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55\n",
      "147/146 [==============================] - 204s 1s/step - loss: 0.7346 - acc: 0.5057 - val_loss: 0.7331 - val_acc: 0.5363\n",
      "Epoch 2/55\n",
      "147/146 [==============================] - 205s 1s/step - loss: 0.7237 - acc: 0.5038 - val_loss: 0.7306 - val_acc: 0.5308\n",
      "Epoch 3/55\n",
      "147/146 [==============================] - 205s 1s/step - loss: 0.7204 - acc: 0.5090 - val_loss: 0.7333 - val_acc: 0.5308\n",
      "Epoch 4/55\n",
      "147/146 [==============================] - 203s 1s/step - loss: 0.7164 - acc: 0.5067 - val_loss: 0.7402 - val_acc: 0.5275\n",
      "Epoch 5/55\n",
      "147/146 [==============================] - 204s 1s/step - loss: 0.7184 - acc: 0.5115 - val_loss: 0.7407 - val_acc: 0.5297\n",
      "Epoch 6/55\n",
      "147/146 [==============================] - 203s 1s/step - loss: 0.7143 - acc: 0.5168 - val_loss: 0.7415 - val_acc: 0.5297\n",
      "Epoch 7/55\n",
      "147/146 [==============================] - 203s 1s/step - loss: 0.7102 - acc: 0.5135 - val_loss: 0.7395 - val_acc: 0.5308\n",
      "Epoch 8/55\n",
      "147/146 [==============================] - 204s 1s/step - loss: 0.7185 - acc: 0.5049 - val_loss: 0.7421 - val_acc: 0.5319\n",
      "Epoch 9/55\n",
      "147/146 [==============================] - 204s 1s/step - loss: 0.7108 - acc: 0.5150 - val_loss: 0.7438 - val_acc: 0.5319\n",
      "Epoch 10/55\n",
      "147/146 [==============================] - 203s 1s/step - loss: 0.7135 - acc: 0.5096 - val_loss: 0.7407 - val_acc: 0.5275\n",
      "Epoch 11/55\n",
      "147/146 [==============================] - 204s 1s/step - loss: 0.7092 - acc: 0.5098 - val_loss: 0.7422 - val_acc: 0.5319\n",
      "Epoch 12/55\n",
      "147/146 [==============================] - 202s 1s/step - loss: 0.7161 - acc: 0.4927 - val_loss: 0.7423 - val_acc: 0.5286\n",
      "Epoch 13/55\n",
      "147/146 [==============================] - 204s 1s/step - loss: 0.7134 - acc: 0.4939 - val_loss: 0.7464 - val_acc: 0.5275\n",
      "Epoch 14/55\n",
      "147/146 [==============================] - 203s 1s/step - loss: 0.7079 - acc: 0.5055 - val_loss: 0.7438 - val_acc: 0.5297\n",
      "Epoch 15/55\n",
      "147/146 [==============================] - 203s 1s/step - loss: 0.7065 - acc: 0.5117 - val_loss: 0.7446 - val_acc: 0.5297\n",
      "Epoch 16/55\n",
      "147/146 [==============================] - 203s 1s/step - loss: 0.7071 - acc: 0.5131 - val_loss: 0.7423 - val_acc: 0.5275\n",
      "Epoch 17/55\n",
      " 92/146 [=================>............] - ETA: 1:09 - loss: 0.7102 - acc: 0.5040"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-433-cb14b4dc4c4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_img\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mval_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    241\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager_learning_phase_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mcurrent_trainable_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trainable_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trainable_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_trainable_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m       outs, total_loss, output_losses, masks = (\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_set_trainable_state\u001b[0;34m(self, trainable_state)\u001b[0m\n\u001b[1;32m   2178\u001b[0m     \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrackable_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_empty_layer_containers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainable_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m       \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trainable_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    430\u001b[0m                            ' Always start with this line.')\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;31m# Keep track of metric instance created in subclassed model/layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2254\u001b[0m         hasattr(self.__class__, name)):\n\u001b[1;32m   2255\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2256\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoTrackable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2257\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2258\u001b[0m         raise AttributeError(\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_layers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    430\u001b[0m                            ' Always start with this line.')\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;31m# Keep track of metric instance created in subclassed model/layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2254\u001b[0m         hasattr(self.__class__, name)):\n\u001b[1;32m   2255\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2256\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoTrackable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2257\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2258\u001b[0m         raise AttributeError(\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mtrainable\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_layers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2254\u001b[0m         hasattr(self.__class__, name)):\n\u001b[1;32m   2255\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2256\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoTrackable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2257\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2258\u001b[0m         raise AttributeError(\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36mtrainable\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_var\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2272\u001b[0m     \u001b[0;31m# if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2273\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2274\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2275\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__delattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2218\u001b[0m     \u001b[0;31m# other attributes referencing it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m     \u001b[0mreference_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj_reference_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2220\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mexisting_value\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreference_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2221\u001b[0m       \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoTrackable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/_collections_abc.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/util/object_identity.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep/lib/python3.7/site-packages/tensorflow_core/python/util/object_identity.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ObjectIdentityWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot compare wrapped object with unwrapped object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history1 = base_model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=tr_img/tr_batch,\n",
    "      epochs=55,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=val_img/val_batch,\n",
    "      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Retrieve a list of accuracy results on training and validation data\n",
    "# sets for each training epoch\n",
    "acc = history1.history['acc']\n",
    "val_acc = history1.history['val_acc']\n",
    "\n",
    "# Retrieve a list of list results on training and validation data\n",
    "# sets for each training epoch\n",
    "loss = history1.history['loss']\n",
    "val_loss = history1.history['val_loss']\n",
    "\n",
    "# Get number of epochs\n",
    "epochs = range(len(acc))\n",
    "\n",
    "# Plot training and validation accuracy per epoch\n",
    "plt.plot(epochs, acc)\n",
    "plt.plot(epochs, val_acc)\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plot training and validation loss per epoch\n",
    "plt.plot(epochs, loss)\n",
    "plt.plot(epochs, val_loss)\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
